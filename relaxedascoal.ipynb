{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceType":"datasetVersion","sourceId":13765647,"datasetId":8760569,"databundleVersionId":14517909,"isSourceIdPinned":false},{"sourceType":"datasetVersion","sourceId":13756248,"datasetId":8753670,"databundleVersionId":14507408},{"sourceType":"datasetVersion","sourceId":13756257,"datasetId":8753679,"databundleVersionId":14507420,"isSourceIdPinned":false},{"sourceType":"datasetVersion","sourceId":13756266,"datasetId":8753686,"databundleVersionId":14507431,"isSourceIdPinned":false},{"sourceType":"datasetVersion","sourceId":5601426,"datasetId":3222255,"databundleVersionId":5676449,"isSourceIdPinned":false},{"sourceType":"modelInstanceVersion","sourceId":647815,"databundleVersionId":14507444,"modelInstanceId":488616},{"sourceType":"modelInstanceVersion","sourceId":647814,"databundleVersionId":14507438,"modelInstanceId":488615},{"sourceType":"modelInstanceVersion","sourceId":647986,"databundleVersionId":14509375,"modelInstanceId":488764}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Cell 1 — Environment & Data Setup\n\nThis cell prepares the workspace for the project:\n\n* Creates a local data/ directory to store all dataset files.\n\n* Downloads a dataset from Google Drive .\n\n* Clones the required GitHub repository (challenge) which contains additional code and utilities used throughout this notebook (evaluation or model and dataset loading functions).\n\nTo load translator of the submission go to \"Model, Training, and Dataset Parameters\" section and set:\n-TRANSLATOR_PATH = \"/kaggle/input/translator903/other/default/1/translator.pth\"","metadata":{}},{"cell_type":"code","source":"import kagglehub\nprint(\"Downloading inputs\")\n# Download latest version\npath = kagglehub.dataset_download(\"ferruccioliu/openai-clip-vit-large-patch14\")\npath = kagglehub.dataset_download(\"niccolosici/newdataset40000-50000\")\npath = kagglehub.dataset_download(\"niccolosici/newdataset10000-20000\")\npath = kagglehub.dataset_download(\"niccolosici/aml-dataset\")\npath = kagglehub.dataset_download(\"niccolosici/newdataset\")\npath = kagglehub.model_download(\"niccolosici/translator903/other/default\")\npath = kagglehub.model_download(\"niccolosici/dec903/other/default\")\npath = kagglehub.model_download(\"niccolosici/enc903/other/default\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T12:45:22.562800Z","iopub.execute_input":"2025-11-17T12:45:22.563069Z","iopub.status.idle":"2025-11-17T12:45:26.066254Z","shell.execute_reply.started":"2025-11-17T12:45:22.563047Z","shell.execute_reply":"2025-11-17T12:45:26.065449Z"}},"outputs":[{"name":"stdout","text":"Downloading inputs\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!mkdir data\n!gdown 1CVAQDuPOiwm8h9LJ8a_oOs6zOWS6EgkB\n!gdown 1ykZ9fjTxUwdiEwqagoYZiMcD5aG-7rHe\n!unzip -o test.zip -d data\n!unzip -o train.zip -d data\n!git clone https://github.com/Mamiglia/challenge.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T12:45:26.068654Z","iopub.execute_input":"2025-11-17T12:45:26.068911Z","iopub.status.idle":"2025-11-17T12:45:34.886210Z","shell.execute_reply.started":"2025-11-17T12:45:26.068891Z","shell.execute_reply":"2025-11-17T12:45:34.885277Z"}},"outputs":[{"name":"stdout","text":"Downloading...\nFrom: https://drive.google.com/uc?id=1CVAQDuPOiwm8h9LJ8a_oOs6zOWS6EgkB\nTo: /kaggle/working/test.zip\n100%|██████████████████████████████████████| 5.80M/5.80M [00:00<00:00, 18.8MB/s]\nFailed to retrieve file url:\n\n\tToo many users have viewed or downloaded this file recently. Please\n\ttry accessing the file again later. If the file you are trying to\n\taccess is particularly large or is shared with many people, it may\n\ttake up to 24 hours to be able to view or download the file. If you\n\tstill can't access a file after 24 hours, contact your domain\n\tadministrator.\n\nYou may still be able to access the file from the browser:\n\n\thttps://drive.google.com/uc?id=1ykZ9fjTxUwdiEwqagoYZiMcD5aG-7rHe\n\nbut Gdown can't. Please check connections and permissions.\nArchive:  test.zip\n   creating: data/test/\n  inflating: data/test/captions.txt  \n  inflating: data/test/test.clean.npz  \nunzip:  cannot find or open train.zip, train.zip.zip or train.zip.ZIP.\nCloning into 'challenge'...\nremote: Enumerating objects: 98, done.\u001b[K\nremote: Counting objects: 100% (98/98), done.\u001b[K\nremote: Compressing objects: 100% (69/69), done.\u001b[K\nremote: Total 98 (delta 39), reused 72 (delta 26), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (98/98), 21.03 MiB | 16.20 MiB/s, done.\nResolving deltas: 100% (39/39), done.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport torch.nn.functional as F\nimport random\nfrom challenge.src.common import load_data, prepare_train_data, generate_submission\nfrom challenge.src.eval import evaluate_retrieval\nimport numpy as np\nimport math\nimport pickle\n\nfrom transformers import CLIPModel,CLIPProcessor\nimport torch\nimport numpy as np\nfrom tqdm import tqdm\nimport transformers.utils.hub as hub_utils \nfrom PIL import Image\nimport pandas as pd\nfrom collections import defaultdict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T12:45:34.887336Z","iopub.execute_input":"2025-11-17T12:45:34.887622Z","iopub.status.idle":"2025-11-17T12:46:05.900685Z","shell.execute_reply.started":"2025-11-17T12:45:34.887597Z","shell.execute_reply":"2025-11-17T12:46:05.900054Z"}},"outputs":[{"name":"stderr","text":"2025-11-17 12:45:47.609492: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763383547.810463      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763383547.866797      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"}],"execution_count":3},{"cell_type":"code","source":"\ndef compute_mrr_at_k_batched(text_proj, image_emb, k=100, batch_size=128):\n    text_proj = F.normalize(text_proj, dim=-1)\n    image_emb = F.normalize(image_emb, dim=-1)\n    \n    N = text_proj.shape[0]\n    reciprocal_ranks = []\n    \n    with torch.no_grad():\n        for start in range(0, N, batch_size):\n            end = min(start + batch_size, N)\n            batch_t = text_proj[start:end]\n            \n            sims = torch.matmul(batch_t, image_emb.T)\n            top_k_values, top_k_indices = torch.topk(sims, k=min(k, N), dim=1)\n            \n            for i in range(end - start):\n                true_idx = start + i\n                top_k_for_query = top_k_indices[i].cpu().numpy()\n                position = (top_k_for_query == true_idx).nonzero()\n                \n                if len(position[0]) > 0:\n                    rank = position[0][0] + 1\n                    reciprocal_ranks.append(1.0 / rank)\n                else:\n                    reciprocal_ranks.append(0.0)\n    \n    return sum(reciprocal_ranks) / len(reciprocal_ranks)\n\n\ndef compute_recall_at_k(text_proj, image_emb, k=1, batch_size=128):\n    text_proj = F.normalize(text_proj, dim=-1)\n    image_emb = F.normalize(image_emb, dim=-1)\n    \n    N = text_proj.shape[0]\n    correct = 0\n    \n    with torch.no_grad():\n        for start in range(0, N, batch_size):\n            end = min(start + batch_size, N)\n            batch_t = text_proj[start:end]\n            \n            sims = torch.matmul(batch_t, image_emb.T)\n            top_k_indices = torch.topk(sims, k=min(k, N), dim=1)[1]\n            \n            for i in range(end - start):\n                true_idx = start + i\n                if true_idx in top_k_indices[i].cpu().numpy():\n                    correct += 1\n    \n    return correct / N","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T12:46:05.902476Z","iopub.execute_input":"2025-11-17T12:46:05.902998Z","iopub.status.idle":"2025-11-17T12:46:05.910973Z","shell.execute_reply.started":"2025-11-17T12:46:05.902979Z","shell.execute_reply":"2025-11-17T12:46:05.910269Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Main Dataset Construction Components\n\nThis cell defines the two core functions responsible for preparing the datasets used in the two-stage training pipeline:\n- Stage 1: Building the Encoder Dataset\n- Stage 2: Building the Decoder Dataset\n\n### extract_dataset_CLIP_EM() — Building the Encoder Training Dataset\n\nPrepares the dataset used to train the Stage 1 encoder MLP, whose purpose is to map external caption embeddings (e.g., SBERT, DINO, or other text encoders) into the CLIP text-embedding space:\n- Loads existing caption embeddings from the original dataset.\n- Loads several external datasets containing additional caption embeddings and caption texts.\n- Merges all caption embeddings into one unified collection.\n- Uses a local CLIP model to compute CLIP text embeddings for every caption (these become the targets).\n- Optionally applies data augmentation at the embedding level (noise, dropout, mixup).\n- Splits the dataset into training and validation sets.\n- Builds PyTorch DataLoader objects for efficient batching.\n\n### extract_dataset_from() — Building the Decoder Training Dataset\n\nPrepares the dataset used to train the Stage 2 decoder MLP, whose role is to map CLIP-aligned caption embeddings into the image embedding space:\n- Loads the original training dataset (with image embeddings and caption embeddings).\n- Loads multiple external datasets containing additional image and caption embeddings.\n- Concatenates all old and new caption embeddings into one dataset.\n- Uses the previously trained encoder model (from Stage 1) to convert caption embeddings into the CLIP-aligned space.\n- Matches each caption embedding to the correct image embedding (targets).\n- Optionally applies augmentation on the caption embeddings.\n- Splits the transformed dataset into training and validation sets.\n- Wraps everything into PyTorch dataloaders.\n","metadata":{}},{"cell_type":"code","source":"def load_data_from_dataset_img_emb(new_dataset_file = \"/kaggle/input/newdataset/coco_sbert_dino.pkl\"):\n    \n    with open(new_dataset_file, \"rb\") as f:\n        new_dataset = pickle.load(f)\n    \n    new_caption_emb = np.vstack([entry['caption_embedding'] for entry in new_dataset])\n    new_image_emb_np = np.stack([entry[\"img_embedding\"] for entry in new_dataset])  # shape: (N_new, 1536)\n    return new_caption_emb, torch.from_numpy(new_image_emb_np).float()\n\ndef extract_dataset_from(dataset_path,dataset_par, augmentation_par,encoder_model_par, encoder_path):\n\n    train_data = load_data(dataset_path)\n    \n    TRAIN_SIZE = dataset_par[\"TRAIN_SIZE\"]\n    BATCH_SIZE = dataset_par[\"BATCH_SIZE\"]\n    USE_AUGMENTATION =  augmentation_par[\"USE_AUGMENTATION\"]\n    NUM_AUGMENTED_COPIES = augmentation_par[\"NUM_AUGMENTED_COPIES\"]\n    NOISE_STD = augmentation_par[\"NOISE_STD\"]\n    DROPOUT_AUG_PROB = augmentation_par[\"DROPOUT_AUG_PROB\"]\n    MIXUP_ALPHA = augmentation_par[\"MIXUP_ALPHA\"]\n    MIXUP_PROB_BATCH = augmentation_par[\"MIXUP_PROB_BATCH\"]\n    \n    print(\"\\nLoading data...\")\n    \n    new_caption_emb0,new_image_emb0 = load_data_from_dataset_img_emb(\"/kaggle/input/newdataset/coco_sbert_dino.pkl\")\n    new_caption_emb1,new_image_emb1 = load_data_from_dataset_img_emb(\"/kaggle/input/newdataset10000-20000/coco_sbert_dino_10000_to_20000.pkl\")\n    new_caption_emb2,new_image_emb2 = load_data_from_dataset_img_emb(\"/kaggle/input/newdataset40000-50000/coco_sbert_dino_40000_to_50000.pkl\")\n    \n    \n    new_caption_emb = [new_caption_emb0,new_caption_emb1,new_caption_emb2]\n    new_image_emb = [new_image_emb0, new_image_emb1,new_image_emb2]\n\n    # Convert to torch tensor\n    \n    old_embeddings = train_data['captions/embeddings']\n    image_embeddings = torch.from_numpy(train_data['images/embeddings']).float()\n\n    old_embeddings = np.vstack([old_embeddings]+new_caption_emb)\n    \n    old_embeddings = torch.from_numpy(old_embeddings).float()\n    caption_label = train_data['captions/label']\n    \n    caption_to_image_idx = np.argmax(caption_label, axis=1)\n    target_image_embeddings = image_embeddings[caption_to_image_idx]\n\n    target_image_embeddings = torch.cat([target_image_embeddings]+ new_image_emb, dim=0)\n    encoder_model = encoder_model_par[\"CREATE_MODEL\"](encoder_model_par)\n    encoder_model.load_state_dict(torch.load(encoder_path))\n    encoder_model.eval()\n\n    \n    print(\"Transforming embeddings through Stage 1...\")\n    with torch.no_grad():\n        batch_size_transform = 10000\n        clip_embeddings_list = []\n    \n        for i in tqdm(range(0, len(old_embeddings), batch_size_transform)):\n            batch = old_embeddings[i:i+batch_size_transform].to(DEVICE)\n            clip_batch = encoder_model(batch)\n            clip_embeddings_list.append(clip_batch.cpu())\n    \n        clip_embeddings = torch.cat(clip_embeddings_list, dim=0)\n    \n    \n    n_train = len(clip_embeddings)-int((len(clip_embeddings)*0.1 // 100) * 100)\n    indices = torch.randperm(len(clip_embeddings))\n   \n    X_train_orig, X_val = clip_embeddings[indices[:n_train]], clip_embeddings[indices[n_train:]]\n    y_train_orig, y_val = target_image_embeddings[indices[:n_train]], target_image_embeddings[indices[n_train:]]\n    \n    print(f\"Original Train: {len(X_train_orig)}, Val: {len(X_val)}\")\n    \n    # Initialize augmenter\n    X_train, y_train = X_train_orig, y_train_orig\n    \n    if USE_AUGMENTATION and NUM_AUGMENTED_COPIES > 0:\n        augmenter = EmbeddingAugmenter(\n            noise_std=NOISE_STD,\n            dropout_prob=DROPOUT_AUG_PROB,\n            mixup_alpha=MIXUP_ALPHA\n        )\n        print(f\"\\n Augmentation Configuration:\")\n        print(f\"   - Augmented copies per sample: {NUM_AUGMENTED_COPIES}\")\n        print(f\"   - Gaussian noise: std={NOISE_STD}\")\n        print(f\"   - Embedding dropout: prob={DROPOUT_AUG_PROB}\")\n        print(f\"   - Mixup (batch-level): alpha={MIXUP_ALPHA}, prob={MIXUP_PROB_BATCH}\")\n        \n        # Create augmented training set\n        X_train, y_train = create_augmented_dataset(\n            X_train_orig, \n            y_train_orig, \n            augmenter, \n            NUM_AUGMENTED_COPIES\n        )\n    else:\n        print(\"\\n Augmentation disabled\")\n    \n    print(f\"\\n Final Training Size: {len(X_train)}\")\n    print(f\" Validation Size: {len(X_val)} (no augmentation)\")\n    \n    # Clean up CLIP model to free memory\n    # The original code had clip_tokenizer here, but it's not defined. Removed it.\n    torch.cuda.empty_cache()\n    \n    # Create datasets\n    train_dataset = TensorDataset(X_train, y_train)\n    val_dataset = TensorDataset(X_val, y_val)\n    \n    # Dataloaders\n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, \n                             num_workers=0, pin_memory=True)\n    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, \n                           num_workers=0, pin_memory=True)\n    \n    return train_loader, val_loader\n\n\n\n\ndef image_caption_lists():\n    captions_file = \"/kaggle/input/aml-competition/train/train/captions.txt\"\n    \n    # Read the file\n    df = pd.read_csv(captions_file, sep=',', header=None, names=['image', 'caption'])\n    \n    # Get lists directly from dataframe (preserves duplicates and order)\n    image_names = df['image'].tolist()\n    text_captions = df['caption'].tolist()\n    \n    return image_names[1:], text_captions[1:]\ndef load_img_clip_dict():\n    file_pt= \"/kaggle/input/img-to-clip/y_clip(1).npz\"\n    \n    # Load the saved .npz file\n    data = np.load(file_pt, allow_pickle=True)\n    \n    # Extract arrays\n    embeddings = data['embeddings']       # shape: (N, D)\n    image_names = data['image_names']     # shape: (N,)\n    \n    # Convert image names to plain Python strings (if they’re bytes)\n    image_names = [str(name) for name in image_names]\n    \n    # Create dictionary: {image_name: emb_vec}\n    image_to_emb = {\n        img_name: torch.from_numpy(emb_vec).float() \n        for img_name, emb_vec in zip(image_names, embeddings)\n    }\n    return image_to_emb\ndef load_data_from_dataset(new_dataset_file = \"/kaggle/input/newdataset/coco_sbert_dino.pkl\"):\n    \n    with open(new_dataset_file, \"rb\") as f:\n        new_dataset = pickle.load(f)\n    \n    new_caption_emb = np.vstack([entry['caption_embedding'] for entry in new_dataset])\n    new_caption_texts = [entry['caption'] for entry in new_dataset]\n    return new_caption_emb, new_caption_texts\n    \ndef extract_dataset_CLIP_EM(dataset_path, dataset_par, augmentation_par, clip_model_name=\"openai/clip-vit-large-patch14-336\"):\n    print(\"NEW vertotot\")\n    \"\"\"\n    Extract dataset with pre-existing caption embeddings as input (X) \n    and CLIP-generated embeddings as targets (y)\n    \n    Args:\n        dataset_path: Path to dataset\n        dataset_par: Dataset parameters\n        augmentation_par: Augmentation parameters\n        clip_model_name: CLIP model to use\n    \n    Returns:\n        train_loader, val_loader with (caption_emb, clip_text_emb) pairs\n    \"\"\"\n    \n    TRAIN_SIZE = dataset_par[\"TRAIN_SIZE\"]\n    BATCH_SIZE = dataset_par[\"BATCH_SIZE\"]\n    USE_AUGMENTATION = augmentation_par[\"USE_AUGMENTATION\"]\n    NUM_AUGMENTED_COPIES = augmentation_par[\"NUM_AUGMENTED_COPIES\"]\n    NOISE_STD = augmentation_par[\"NOISE_STD\"]\n    DROPOUT_AUG_PROB = augmentation_par[\"DROPOUT_AUG_PROB\"]\n    MIXUP_ALPHA = augmentation_par[\"MIXUP_ALPHA\"]\n    MIXUP_PROB_BATCH = augmentation_par[\"MIXUP_PROB_BATCH\"]\n    \n    \n    new_caption_emb0,new_caption_texts0 = load_data_from_dataset(\"/kaggle/input/newdataset/coco_sbert_dino.pkl\")\n    new_caption_emb1,new_caption_texts1 = load_data_from_dataset(\"/kaggle/input/newdataset10000-20000/coco_sbert_dino_10000_to_20000.pkl\")\n    new_caption_emb2,new_caption_texts2 = load_data_from_dataset(\"/kaggle/input/newdataset40000-50000/coco_sbert_dino_40000_to_50000.pkl\")\n    \n    new_caption_emb = [new_caption_emb0,new_caption_emb1, new_caption_emb2]\n    new_caption_texts = new_caption_texts0+ new_caption_texts1+new_caption_texts2\n    \n    train_data = np.load(dataset_path)\n    \n    old_embeddings = train_data['captions/embeddings']\n    caption_text = train_data['captions/text']\n    \n    print(f\"Old embeddings: {old_embeddings.shape}\")\n    print(f\"Captions: {len(caption_text)}\")\n\n    old_embeddings = np.vstack([old_embeddings]+ new_caption_emb)\n    caption_text  =list(caption_text) + new_caption_texts\n    \n    print(\"Concatenated shapes:\", len(old_embeddings), len(caption_text))\n\n    print(\"\\nGenerating CLIP embeddings...\")\n    model_clip = CLIPModel.from_pretrained(clip_model_name,local_files_only=True)\n    processor = CLIPProcessor.from_pretrained(clip_model_name,local_files_only=True)\n    model_clip = model_clip.to(DEVICE)\n    model_clip.eval()\n    \n    all_clip_embeddings = []\n    caption_list = [str(cap) for cap in caption_text]\n    \n    with torch.no_grad():\n        for i in tqdm(range(0, len(caption_list), 256), desc=\"CLIP embeddings\"):\n            batch_captions = caption_list[i:i+256]\n            inputs = processor(text=batch_captions, return_tensors=\"pt\", padding=True,\n                              truncation=True, max_length=77).to(DEVICE)\n            text_embeddings = model_clip.get_text_features(**inputs)\n            text_embeddings = text_embeddings / text_embeddings.norm(dim=-1, keepdim=True)\n            all_clip_embeddings.append(text_embeddings.cpu())\n    \n            if (i // BATCH_SIZE) % 10 == 0:\n                torch.cuda.empty_cache()\n    \n    clip_embeddings = torch.cat(all_clip_embeddings, dim=0)\n    print(f\"Generated CLIP embeddings: {clip_embeddings.shape}\")\n    \n    np.savez(\"clip_txt_emb.npz\", clip_embeddings=clip_embeddings.numpy(),\n             caption_ids=train_data['captions/ids'] if 'captions/ids' in train_data else np.arange(len(clip_embeddings)))\n    \n    del model_clip, processor\n    torch.cuda.empty_cache()\n    \n    print(\"\\nTraining Stage 1 MLP...\")\n    X = torch.from_numpy(old_embeddings).float()\n    y = clip_embeddings.float()\n    \n    n_train = len(X)-int((len(X)*0.1 // 100) * 100)\n    indices = torch.randperm(len(X))\n    X_train, X_val = X[indices[:n_train]], X[indices[n_train:]]\n    y_train, y_val = y[indices[:n_train]], y[indices[n_train:]]\n    \n    train_dataset = TensorDataset(X_train, y_train)\n    val_dataset = TensorDataset(X_val, y_val)\n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, num_workers=0, pin_memory=True)\n    return train_loader, val_loader\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T12:46:05.911804Z","iopub.execute_input":"2025-11-17T12:46:05.912056Z","iopub.status.idle":"2025-11-17T12:46:05.952077Z","shell.execute_reply.started":"2025-11-17T12:46:05.912030Z","shell.execute_reply":"2025-11-17T12:46:05.951205Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"##  Loss Functions for Text-Image Embeddings\n\nThis cell defines loss functions for embeddings alignedment:\n\n- `symmetric_contrastive_loss`: computes contrastive loss between text and image embeddings in both directions.  \n- `cosine_loss`: computes the mean cosine similarity loss between predicted and target embeddings.  \n- `triplet_loss`: computes triplet loss with hardest negative mining.  \n- `combined_loss`: combines contrastive and triplet losses with a weighted sum using specified hyperparameters.\n\nThe cosine loss is used by the encoder to map the CLIP text encoder embedding space. While the combined loss is used by the decoder to map CLIP embedding into 1536 dimesnional image embedding space\n","metadata":{}},{"cell_type":"code","source":"\n\ndef symmetric_contrastive_loss(text_proj, image_emb, temperature=0.05):\n    text_proj = F.normalize(text_proj, dim=-1)\n    image_emb = F.normalize(image_emb, dim=-1)\n    logits = torch.matmul(text_proj, image_emb.T) / temperature\n    batch_size = logits.shape[0]\n    labels = torch.arange(batch_size, device=logits.device)\n    loss_t2i = F.cross_entropy(logits, labels)\n    loss_i2t = F.cross_entropy(logits.T, labels)\n    return (loss_t2i + loss_i2t) / 2\n\ndef cosine_loss(pred, target, loss_arg=None):\n    pred_norm = F.normalize(pred, dim=-1)\n    target_norm = F.normalize(target, dim=-1)\n    cos_sim = (pred_norm * target_norm).sum(dim=-1)\n    return (1 - cos_sim).mean()\n\n\n    \ndef triplet_loss(text_proj, image_emb, margin=0.4):\n    text_proj = F.normalize(text_proj, dim=-1)\n    image_emb = F.normalize(image_emb, dim=-1)\n    batch_size = text_proj.shape[0]\n    sims = torch.matmul(text_proj, image_emb.T)\n    pos_sims = sims.diagonal()\n    mask = 1.0 - torch.eye(batch_size, device=sims.device)\n    neg_sims = sims * mask + torch.eye(batch_size, device=sims.device) * -1e9\n    hard_neg_sims, _ = neg_sims.max(dim=1)\n    loss = F.relu(margin - pos_sims + hard_neg_sims).mean()\n    return loss\n\n\ndef combined_loss(text_proj, image_emb, loss_arg):\n    temperature=loss_arg[\"TEMPERATURE\"]\n    alpha=loss_arg[\"ALPHA\"]\n    margin=loss_arg[\"MARGIN\"]\n    contrastive =  symmetric_contrastive_loss(text_proj, image_emb, temperature)\n    triplet = triplet_loss(text_proj, image_emb, margin=margin)\n    \n    return contrastive*alpha + (1-alpha)*triplet\n    #return (text_proj - image_emb).norm(dim=1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T12:46:05.952966Z","iopub.execute_input":"2025-11-17T12:46:05.953192Z","iopub.status.idle":"2025-11-17T12:46:05.973126Z","shell.execute_reply.started":"2025-11-17T12:46:05.953174Z","shell.execute_reply":"2025-11-17T12:46:05.972284Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# MLP and Transformer Model Constructors\n\nThis cell defines functions and classes to create various neural network models used in the pipeline:\n\n-  `create_MLP_contrastive`,`create_Dec`: helper functions to instantiate models with parameters from `model_par` and move them to the device.\n- `ProjectionMLP`: Encoder MLP for projecting embeddings into a shared space, with skip connections and BatchNorm.\n- `DecoderMLP`: decoder MLP mapping latent embeddings to image embeddings, with skip connections.\n- (Commented-out code): alternative residual-block-based `DecoderMLP` architecture.\n","metadata":{}},{"cell_type":"code","source":"\n\ndef create_MLP_contrastive(model_par):\n    return ProjectionMLP(model_par[\"MLP_T_EM\"][\"INPUT_DIM\"], \n                          768, \n                          model_par[\"MLP_T_EM\"][\"HIDDEN_DIMS\"],\n                          model_par[\"MLP_T_EM\"][\"DROPOUT\"]\n    ).to(DEVICE)\n\ndef create_Dec(model_par):\n    return DecoderMLP(\n        input_dim=model_par[\"INPUT_DIM\"],\n        output_dim=model_par[\"OUTPUT_DIM\"],\n        hidden_dims = model_par[\"HIDDEN_DIMS\"],\n        dropout = model_par[\"DROPOUT\"],\n    ).to(DEVICE)\n    \ndef create_AE_Translator(encoder, decoder):\n    return Translator(encoder, decoder).to(DEVICE)\n    \nclass Translator(nn.Module):\n    \n    def __init__(self, encoder, decoder):\n        super().__init__()\n    \n        self.encoder = encoder\n        self.decoder = decoder\n\n        \n    def forward(self, x):\n        x = self.encoder(x)\n        out = self.decoder(x)\n        return out\n\n\nclass ProjectionMLP(nn.Module):\n    \"\"\"MLP for projecting embeddings to shared space\"\"\"\n    \n    def __init__(self, input_dim, output_dim, hidden_dims, dropout):\n        super().__init__()\n        print(\"Creating ProjectionMLP\")\n        layers = []\n        prev_dim = input_dim\n        \n        for i, hidden_dim in enumerate(hidden_dims):\n            layers.extend([\n                nn.Linear(prev_dim, hidden_dim),\n                nn.BatchNorm1d(hidden_dim),\n                nn.GELU(),\n                nn.Dropout(dropout if i < len(hidden_dims) - 1 else dropout * 0.5)\n            ])\n            prev_dim = hidden_dim\n        \n        layers.extend([\n            nn.Linear(prev_dim, output_dim),\n            nn.BatchNorm1d(output_dim)\n        ])\n        \n        self.network = nn.Sequential(*layers)\n        self.skip = nn.Linear(input_dim, output_dim)\n        self.skip_weight = nn.Parameter(torch.tensor(0.1))\n        \n    def forward(self, x):\n        out = self.network(x)\n        out = F.normalize(out, dim=-1)\n        skip = self.skip(x)\n        return out + self.skip_weight * skip\n\n\n\n\n\nclass DecoderMLP(nn.Module):\n    \"\"\"MLP decoder from latent space to image embedding space\"\"\"\n    \n    def __init__(self, input_dim=768, output_dim=1536, hidden_dims=[1024, 1280, 1408], dropout=0.2):\n        super().__init__()\n        # Load encoder (NO FREEZING)\n        print(\"LayerNorm\", hidden_dims)\n        # That's it. Encoder is trainable.\n        \n        layers = []\n        prev_dim = input_dim\n        \n        for i, hidden_dim in enumerate(hidden_dims):\n            layers.extend([\n                nn.Linear(prev_dim, hidden_dim),\n                nn.LayerNorm(hidden_dim),\n                nn.GELU(),\n                nn.Dropout(dropout if i < len(hidden_dims) - 1 else dropout * 0.5)\n            ])\n            prev_dim = hidden_dim\n        \n        layers.extend([nn.Linear(prev_dim, output_dim)])\n        \n        self.network = nn.Sequential(*layers)\n        self.skip = nn.Linear(input_dim, output_dim)\n        self.skip_weight = nn.Parameter(torch.tensor(0.1))\n        \n    def forward(self, x):\n\n        out = self.network(x)\n        out = F.normalize(out, dim=-1)\n        skip = self.skip(x)\n        return out + self.skip_weight * skip\n\ndef create_model(model_par):\n    print(\"\\n2. Building model...\")\n\n    model = model_par[\"CREATE_MODEL\"](model_par)\n    \n    print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T12:46:05.974073Z","iopub.execute_input":"2025-11-17T12:46:05.974347Z","iopub.status.idle":"2025-11-17T12:46:05.989827Z","shell.execute_reply.started":"2025-11-17T12:46:05.974327Z","shell.execute_reply":"2025-11-17T12:46:05.989128Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def train_model(augmentation_par,train_par, loss_par,model, train_loader, val_loader, device, model_path,augmenter=None):\n    LR =train_par[\"LR\"]\n    WEIGHT_DEC = train_par[\"WEIGHT_DEC\"]\n    WARMUP = train_par[\"WARMUP\"]\n    USE_AUGMENTATION = augmentation_par[\"USE_AUGMENTATION\"]\n    MIXUP_PROB_BATCH = augmentation_par[\"MIXUP_PROB_BATCH\"]\n    FLOW = train_par[\"FLOW\"]\n    epochs = train_par[\"EPOCHS\"]\n    \n    optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DEC)\n    warmup_epochs = WARMUP\n    scheduler_warmup = optim.lr_scheduler.LinearLR(optimizer, start_factor=0.1, total_iters=warmup_epochs)\n    scheduler_cosine = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs-warmup_epochs, eta_min=1e-7)\n    \n    best_mrr = 0.0\n    patience_counter = 0\n    patience = 5\n    loss_fn = loss_par[\"FUNC\"]\n    for epoch in range(epochs):\n        model.train()\n        train_loss = 0\n    \n        for X_batch, y_batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n            X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n            optimizer.zero_grad()\n            outputs = model(X_batch)\n            loss = loss_fn(outputs, y_batch,loss_par[\"ARG\"])\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n    \n        train_loss /= len(train_loader)\n    \n        model.eval()\n        val_loss = 0\n        with torch.no_grad():\n            for X_batch, y_batch in val_loader:\n                X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n                outputs = model(X_batch)\n                loss =  loss_fn(outputs, y_batch,loss_par[\"ARG\"])\n                val_loss += loss.item()\n    \n        val_loss /= len(val_loader)\n    \n        if (epoch + 1) % 2 == 0 or epoch == epochs - 1:\n            all_preds = []\n            all_targets = []\n            model.eval()\n            with torch.no_grad():\n                for X_batch, y_batch in val_loader:\n                    X_batch = X_batch.to(DEVICE)\n                    y_batch = y_batch.to(DEVICE)\n                    pred_batch = model(X_batch)\n                    all_preds.append(pred_batch.cpu())\n                    all_targets.append(y_batch.cpu())\n    \n            all_preds = torch.cat(all_preds, dim=0)\n            all_targets = torch.cat(all_targets, dim=0)\n    \n            mrr_100 = compute_mrr_at_k_batched(all_preds.to(DEVICE), all_targets.to(DEVICE), k=100, batch_size=128)\n            model.eval()\n            print(all_preds.shape,all_targets.shape)\n            print(f\"Epoch {epoch+1}: Train={train_loss:.4f}, Val={val_loss:.4f}, MRR@100={mrr_100:.4f}\", evaluate_retrieval(all_preds.cpu(), all_targets.cpu(), np.arange(len(all_preds))))\n            \n            if train_loss > val_loss:\n                best_mrr = mrr_100\n                patience_counter = 0\n                Path(model_path).parent.mkdir(parents=True, exist_ok=True)\n                torch.save(model.state_dict(), model_path)\n                print(f\"  New best: {mrr_100:.4f}\")\n            else:\n                patience_counter += 1\n    \n            del all_preds, all_targets\n            torch.cuda.empty_cache()\n        else:\n            print(f\"Epoch {epoch+1}: Train={train_loss:.4f}, Val={val_loss:.4f}\")\n    \n        if epoch < warmup_epochs:\n            scheduler_warmup.step()\n        else:\n            scheduler_cosine.step()\n    \n        if patience_counter >= patience:\n            print(f\"Early stopping at epoch {epoch+1}\")\n            break\n    \n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T12:46:05.990678Z","iopub.execute_input":"2025-11-17T12:46:05.990954Z","iopub.status.idle":"2025-11-17T12:46:06.009646Z","shell.execute_reply.started":"2025-11-17T12:46:05.990928Z","shell.execute_reply":"2025-11-17T12:46:06.008973Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# Model, Training, and Dataset Parameters\n\nThis cell defines all the **hyperparameters and configuration dictionaries** used in the pipeline:\n\n- **Model parameters:** `MLP_T_EM`,  `MLP_ENC_PAR`, `MLP_DEC_PAR` define encoder and decoder MLP architectures and how they are created.  \n- **Loss parameters:** `ENC_LOSS_PAR` and `DEC_LOSS_PAR` define the loss functions and their arguments for encoder and decoder training.  \n- **Training parameters:** `ENC_TRAINING_PAR` and `DEC_TRAINING_PAR` define learning rates, warmup, epochs, weight decay, and whether to use flow.  \n- **Dataset parameters:** `ENC_DATASET_PAR` and `DEC_DATASET_PAR` define batch sizes and training/validation split ratios.  \n- **Augmentation parameters:** `AUGMENTATION_PAR` controls optional embedding-level augmentations like noise, dropout, and mixup.  \n- **File and device settings:** `DATASET_PATH`, `DEVICE`, and `GRAD_CLIP`.\n","metadata":{}},{"cell_type":"code","source":"#DEFINE PARAMETERS\n#MODEL\n#Contrastive into 780 space\nMLP_T_EM = {\"NAME\":\"ModelTextToEMb\", \"HIDDEN_DIMS\":[1024, 1024, 896],\"INPUT_DIM\":1024, \"OUTPUT_DIM\":768, \"DROPOUT\": 0.2}\nMLP_ENC_PAR = {\"NAME\":\"ModelContrastive\", \"MLP_T_EM\": MLP_T_EM, \"CREATE_MODEL\": create_MLP_contrastive}\n\nMLP_DEC_PAR = {\"NAME\":\"ModelDecoder\", \"HIDDEN_DIMS\":[896, 1000, 1200, 1400],\"INPUT_DIM\":768, \"OUTPUT_DIM\":1536, \"DROPOUT\": 0.2, \"ENC_MODEL_PATH\":\"models/mlp_enc_txt_cos_loss.pth\",\"DEC_MODEL_PATH\":\"models/mlp_dec_txt_cos_loss.pth\", \"ENCODER_PAR\":MLP_ENC_PAR, \"CREATE_MODEL\":create_Dec}\n#Decoder fron space 780\nENC_LOSS_PAR  = {\"NAME\":\"COSINE_LOSS\", \"FUNC\": cosine_loss, \"ARG\":None}\nDEC_LOSS_PAR = {\"NAME\":\"COMB_TRIPLET_AND_CONTR\", \"FUNC\": combined_loss, \"ARG\":{\"TEMPERATURE\":0.007, \"ALPHA\":0.1, \"MARGIN\":0.4}}\n\n#TRAINING\nENC_TRAINING_PAR = {\"LR\":0.002, \"WARMUP\":3, \"EPOCHS\": 30, \"WEIGHT_DEC\":0.01, \"FLOW\":False}\nDEC_TRAINING_PAR = {\"LR\":0.006, \"WARMUP\":3, \"EPOCHS\": 30, \"WEIGHT_DEC\":0.01, \"FLOW\":False}\n\n\n#DATASET\nENC_DATASET_PAR = {\"BATCH_SIZE\":4096*2, \"TRAIN_SIZE\":0.9}\nDEC_DATASET_PAR = {\"BATCH_SIZE\":4096*5, \"TRAIN_SIZE\":0.9}\n\nAUGMENTATION_PAR = {\"USE_AUGMENTATION\":False, \"NUM_AUGMENTED_COPIES\":2, \"NOISE_STD\":0.02, \"DROPOUT_AUG_PROB\":0.1,\"MIXUP_ALPHA\":0.2, \"MIXUP_PROB_BATCH\": 0.3}\n\n\n\n#FILES\nDATASET_PATH = \"/kaggle/input/aml-dataset/train/train/train.npz\"\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nTRANSLATOR_PATH =\"models/translator.pth\"\n\nGRAD_CLIP = None\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T12:46:06.010459Z","iopub.execute_input":"2025-11-17T12:46:06.010811Z","iopub.status.idle":"2025-11-17T12:46:06.028950Z","shell.execute_reply.started":"2025-11-17T12:46:06.010787Z","shell.execute_reply":"2025-11-17T12:46:06.028133Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"## Create Encoder Dataset\n\nThis cell creates the **training and validation dataloaders** for the Stage 1 encoder by calling `extract_dataset_CLIP_EM` with:\n\n- `DATASET_PATH`: path to the raw dataset  \n- `ENC_DATASET_PAR`: dataset parameters (batch size, train/val split)  \n- `AUGMENTATION_PAR`: optional embedding-level augmentations  \n- CLIP model path: to compute CLIP text embeddings  \n","metadata":{}},{"cell_type":"code","source":"# CREATE DATASET\ntrain_loader_clip, val_loader_clip = extract_dataset_CLIP_EM(DATASET_PATH, ENC_DATASET_PAR, AUGMENTATION_PAR,\"/kaggle/input/openai-clip-vit-large-patch14/clip-vit-large-patch14\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T12:46:06.030869Z","iopub.execute_input":"2025-11-17T12:46:06.031259Z","iopub.status.idle":"2025-11-17T12:51:44.597580Z","shell.execute_reply.started":"2025-11-17T12:46:06.031231Z","shell.execute_reply":"2025-11-17T12:51:44.596545Z"}},"outputs":[{"name":"stdout","text":"NEW vertotot\nOld embeddings: (125000, 1024)\nCaptions: 125000\nConcatenated shapes: 275078 275078\n\nGenerating CLIP embeddings...\n","output_type":"stream"},{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\nCLIP embeddings: 100%|██████████| 1075/1075 [05:07<00:00,  3.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Generated CLIP embeddings: torch.Size([275078, 768])\n\nTraining Stage 1 MLP...\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"## Train Encoder Model\n\n- Instantiates the encoder model using `create_model` with `MLP_ENC_PAR`.  \n- Trains the encoder using `train_model` with specified augmentation, training, and loss parameters, along with the encoder dataloaders.  \n- Saves the trained encoder weights to `MLP_DEC_PAR[\"ENC_MODEL_PATH\"]`.  \n- Deletes the model from memory to free GPU resources.\n","metadata":{}},{"cell_type":"code","source":"encoder_model = create_model(MLP_ENC_PAR)\n\nencoder_model = train_model(AUGMENTATION_PAR,ENC_TRAINING_PAR, ENC_LOSS_PAR, encoder_model,train_loader_clip, val_loader_clip, DEVICE, MLP_DEC_PAR[\"ENC_MODEL_PATH\"])\ndel encoder_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T12:51:44.598473Z","iopub.execute_input":"2025-11-17T12:51:44.598708Z","iopub.status.idle":"2025-11-17T12:55:02.585731Z","shell.execute_reply.started":"2025-11-17T12:51:44.598683Z","shell.execute_reply":"2025-11-17T12:55:02.584907Z"}},"outputs":[{"name":"stdout","text":"\n2. Building model...\nCreating ProjectionMLP\nParameters: 4,501,121\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/30: 100%|██████████| 31/31 [00:05<00:00,  5.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Train=0.6193, Val=0.4307\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/30: 100%|██████████| 31/31 [00:05<00:00,  6.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([27500, 768]) torch.Size([27500, 768])\nEpoch 2: Train=0.3096, Val=0.2369, MRR@100=0.5344 {'mrr': 0.9594460624281227, 'ndcg': 0.9695585807895192, 'recall_at_1': 0.9309818181818181, 'recall_at_3': 0.9875636363636363, 'recall_at_5': 0.9937818181818182, 'recall_at_10': 0.9975636363636363, 'recall_at_50': 0.9997090909090909, 'l2_dist': 1.4779030084609985}\n  New best: 0.5344\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/30: 100%|██████████| 31/31 [00:05<00:00,  5.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Train=0.2099, Val=0.1834\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/30: 100%|██████████| 31/31 [00:04<00:00,  6.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([27500, 768]) torch.Size([27500, 768])\nEpoch 4: Train=0.1793, Val=0.1666, MRR@100=0.6550 {'mrr': 0.973597339426007, 'ndcg': 0.9800774088632642, 'recall_at_1': 0.9561090909090909, 'recall_at_3': 0.9901090909090909, 'recall_at_5': 0.9942909090909091, 'recall_at_10': 0.9973818181818181, 'recall_at_50': 0.9996727272727273, 'l2_dist': 1.4246119260787964}\n  New best: 0.6550\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/30: 100%|██████████| 31/31 [00:04<00:00,  6.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5: Train=0.1666, Val=0.1563\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/30: 100%|██████████| 31/31 [00:04<00:00,  6.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([27500, 768]) torch.Size([27500, 768])\nEpoch 6: Train=0.1581, Val=0.1500, MRR@100=0.7250 {'mrr': 0.9836648191857408, 'ndcg': 0.9877039895691545, 'recall_at_1': 0.9725090909090909, 'recall_at_3': 0.9942181818181818, 'recall_at_5': 0.9970909090909091, 'recall_at_10': 0.9987272727272727, 'recall_at_50': 0.9998181818181818, 'l2_dist': 1.4177603721618652}\n  New best: 0.7250\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/30: 100%|██████████| 31/31 [00:04<00:00,  6.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7: Train=0.1527, Val=0.1454\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/30: 100%|██████████| 31/31 [00:04<00:00,  6.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([27500, 768]) torch.Size([27500, 768])\nEpoch 8: Train=0.1486, Val=0.1418, MRR@100=0.7511 {'mrr': 0.9866502150329218, 'ndcg': 0.9899549880791348, 'recall_at_1': 0.9773454545454545, 'recall_at_3': 0.9956, 'recall_at_5': 0.9977454545454545, 'recall_at_10': 0.9987636363636364, 'recall_at_50': 0.9998545454545454, 'l2_dist': 1.4085776805877686}\n  New best: 0.7511\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/30: 100%|██████████| 31/31 [00:04<00:00,  6.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9: Train=0.1447, Val=0.1385\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/30: 100%|██████████| 31/31 [00:05<00:00,  6.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([27500, 768]) torch.Size([27500, 768])\nEpoch 10: Train=0.1420, Val=0.1359, MRR@100=0.7727 {'mrr': 0.9883567936598342, 'ndcg': 0.9912438904471653, 'recall_at_1': 0.9801090909090909, 'recall_at_3': 0.9962181818181818, 'recall_at_5': 0.9980363636363636, 'recall_at_10': 0.9989818181818182, 'recall_at_50': 0.9998181818181818, 'l2_dist': 1.378578782081604}\n  New best: 0.7727\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/30: 100%|██████████| 31/31 [00:04<00:00,  6.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11: Train=0.1393, Val=0.1338\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/30: 100%|██████████| 31/31 [00:04<00:00,  6.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([27500, 768]) torch.Size([27500, 768])\nEpoch 12: Train=0.1372, Val=0.1317, MRR@100=0.7897 {'mrr': 0.9899128055110019, 'ndcg': 0.9924102002233594, 'recall_at_1': 0.9827272727272728, 'recall_at_3': 0.9967636363636364, 'recall_at_5': 0.9982545454545455, 'recall_at_10': 0.9988727272727272, 'recall_at_50': 0.9998545454545454, 'l2_dist': 1.3487788438796997}\n  New best: 0.7897\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/30: 100%|██████████| 31/31 [00:04<00:00,  6.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13: Train=0.1350, Val=0.1298\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/30: 100%|██████████| 31/31 [00:04<00:00,  6.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([27500, 768]) torch.Size([27500, 768])\nEpoch 14: Train=0.1334, Val=0.1282, MRR@100=0.7965 {'mrr': 0.9897978690495881, 'ndcg': 0.992329038556231, 'recall_at_1': 0.9824727272727273, 'recall_at_3': 0.9968, 'recall_at_5': 0.9982909090909091, 'recall_at_10': 0.9991636363636364, 'recall_at_50': 0.9998545454545454, 'l2_dist': 1.323569655418396}\n  New best: 0.7965\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/30: 100%|██████████| 31/31 [00:04<00:00,  6.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15: Train=0.1315, Val=0.1268\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/30: 100%|██████████| 31/31 [00:04<00:00,  6.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([27500, 768]) torch.Size([27500, 768])\nEpoch 16: Train=0.1301, Val=0.1255, MRR@100=0.8098 {'mrr': 0.9913270416280935, 'ndcg': 0.993471662878524, 'recall_at_1': 0.9851636363636364, 'recall_at_3': 0.9971272727272728, 'recall_at_5': 0.9984, 'recall_at_10': 0.9992, 'recall_at_50': 0.999890909090909, 'l2_dist': 1.2947150468826294}\n  New best: 0.8098\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/30: 100%|██████████| 31/31 [00:04<00:00,  6.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17: Train=0.1285, Val=0.1241\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/30: 100%|██████████| 31/31 [00:04<00:00,  6.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([27500, 768]) torch.Size([27500, 768])\nEpoch 18: Train=0.1273, Val=0.1233, MRR@100=0.8139 {'mrr': 0.991759578990813, 'ndcg': 0.9938004527428591, 'recall_at_1': 0.9858545454545454, 'recall_at_3': 0.9975272727272727, 'recall_at_5': 0.9985818181818182, 'recall_at_10': 0.9992363636363636, 'recall_at_50': 0.9998545454545454, 'l2_dist': 1.2777440547943115}\n  New best: 0.8139\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/30: 100%|██████████| 31/31 [00:04<00:00,  6.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19: Train=0.1263, Val=0.1225\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/30: 100%|██████████| 31/31 [00:04<00:00,  6.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([27500, 768]) torch.Size([27500, 768])\nEpoch 20: Train=0.1252, Val=0.1217, MRR@100=0.8203 {'mrr': 0.9921074257901399, 'ndcg': 0.9940662092606237, 'recall_at_1': 0.9864363636363637, 'recall_at_3': 0.9977454545454545, 'recall_at_5': 0.9986545454545455, 'recall_at_10': 0.9993090909090909, 'recall_at_50': 0.999890909090909, 'l2_dist': 1.2656447887420654}\n  New best: 0.8203\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21/30: 100%|██████████| 31/31 [00:04<00:00,  6.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 21: Train=0.1246, Val=0.1210\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22/30: 100%|██████████| 31/31 [00:05<00:00,  6.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([27500, 768]) torch.Size([27500, 768])\nEpoch 22: Train=0.1235, Val=0.1203, MRR@100=0.8240 {'mrr': 0.992204534128123, 'ndcg': 0.9941358100264702, 'recall_at_1': 0.9865818181818182, 'recall_at_3': 0.9977090909090909, 'recall_at_5': 0.9986545454545455, 'recall_at_10': 0.9993090909090909, 'recall_at_50': 0.999890909090909, 'l2_dist': 1.2479870319366455}\n  New best: 0.8240\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23/30: 100%|██████████| 31/31 [00:05<00:00,  6.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 23: Train=0.1228, Val=0.1198\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24/30: 100%|██████████| 31/31 [00:05<00:00,  6.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([27500, 768]) torch.Size([27500, 768])\nEpoch 24: Train=0.1222, Val=0.1194, MRR@100=0.8277 {'mrr': 0.9920826748762205, 'ndcg': 0.9940434520086442, 'recall_at_1': 0.9864363636363637, 'recall_at_3': 0.9974545454545455, 'recall_at_5': 0.9985818181818182, 'recall_at_10': 0.9993090909090909, 'recall_at_50': 0.999890909090909, 'l2_dist': 1.238354206085205}\n  New best: 0.8277\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25/30: 100%|██████████| 31/31 [00:04<00:00,  6.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 25: Train=0.1216, Val=0.1190\n","output_type":"stream"},{"name":"stderr","text":"Epoch 26/30: 100%|██████████| 31/31 [00:04<00:00,  6.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([27500, 768]) torch.Size([27500, 768])\nEpoch 26: Train=0.1212, Val=0.1188, MRR@100=0.8290 {'mrr': 0.9922682333523836, 'ndcg': 0.9941832706074266, 'recall_at_1': 0.9867272727272727, 'recall_at_3': 0.9976727272727273, 'recall_at_5': 0.9986181818181818, 'recall_at_10': 0.9993090909090909, 'recall_at_50': 0.999890909090909, 'l2_dist': 1.2339528799057007}\n  New best: 0.8290\n","output_type":"stream"},{"name":"stderr","text":"Epoch 27/30: 100%|██████████| 31/31 [00:04<00:00,  6.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 27: Train=0.1208, Val=0.1186\n","output_type":"stream"},{"name":"stderr","text":"Epoch 28/30: 100%|██████████| 31/31 [00:04<00:00,  6.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([27500, 768]) torch.Size([27500, 768])\nEpoch 28: Train=0.1206, Val=0.1185, MRR@100=0.8310 {'mrr': 0.9924028156160418, 'ndcg': 0.9942853365210274, 'recall_at_1': 0.9869454545454546, 'recall_at_3': 0.9977818181818182, 'recall_at_5': 0.9986181818181818, 'recall_at_10': 0.9993090909090909, 'recall_at_50': 0.9998545454545454, 'l2_dist': 1.2293031215667725}\n  New best: 0.8310\n","output_type":"stream"},{"name":"stderr","text":"Epoch 29/30: 100%|██████████| 31/31 [00:04<00:00,  6.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 29: Train=0.1205, Val=0.1184\n","output_type":"stream"},{"name":"stderr","text":"Epoch 30/30: 100%|██████████| 31/31 [00:04<00:00,  6.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([27500, 768]) torch.Size([27500, 768])\nEpoch 30: Train=0.1203, Val=0.1184, MRR@100=0.8315 {'mrr': 0.9923810542736158, 'ndcg': 0.9942691468197736, 'recall_at_1': 0.986909090909091, 'recall_at_3': 0.9977454545454545, 'recall_at_5': 0.9986181818181818, 'recall_at_10': 0.9992727272727273, 'recall_at_50': 0.9998545454545454, 'l2_dist': 1.2275773286819458}\n  New best: 0.8315\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## Create Decoder Dataset\n\nThis cell creates the **training and validation dataloaders** for the Stage 2 decoder by calling `extract_dataset_from` with:\n\n- `DATASET_PATH`: path to the raw dataset  \n- `DEC_DATASET_PAR`: dataset parameters (batch size, train/val split)  \n- `AUGMENTATION_PAR`: optional embedding-level augmentations  \n- `MLP_ENC_PAR` and `MLP_DEC_PAR[\"ENC_MODEL_PATH\"]`: use the trained encoder to convert captions into CLIP-aligned embeddings for the decoder.  \n","metadata":{}},{"cell_type":"code","source":"train_loader_dec, val_loader_dec = extract_dataset_from(DATASET_PATH, DEC_DATASET_PAR, AUGMENTATION_PAR,MLP_ENC_PAR, MLP_DEC_PAR[\"ENC_MODEL_PATH\"] )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T12:55:02.586604Z","iopub.execute_input":"2025-11-17T12:55:02.586888Z","iopub.status.idle":"2025-11-17T12:55:24.177271Z","shell.execute_reply.started":"2025-11-17T12:55:02.586866Z","shell.execute_reply":"2025-11-17T12:55:24.176496Z"}},"outputs":[{"name":"stdout","text":"\nLoading data...\nCreating ProjectionMLP\nTransforming embeddings through Stage 1...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 28/28 [00:01<00:00, 22.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"Original Train: 247578, Val: 27500\n\n Augmentation disabled\n\n Final Training Size: 247578\n Validation Size: 27500 (no augmentation)\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## Train Decoder Model\n\n- Instantiates the decoder model using `create_model` with `MLP_DEC_PAR`.  \n- Trains the decoder using `train_model` with specified augmentation, training, and loss parameters, along with the decoder dataloaders.  \n- Saves the trained decoder weights to `MLP_DEC_PAR[\"DEC_MODEL_PATH\"]`.  \n- Deletes the model from memory to free GPU resources.\n\nAfter training the encoder is loaded and decoder and encoder are saved into one model called translator","metadata":{}},{"cell_type":"code","source":"decoder_model = create_model(MLP_DEC_PAR)\n\ndecoder_model = train_model(AUGMENTATION_PAR,DEC_TRAINING_PAR, DEC_LOSS_PAR, decoder_model,train_loader_dec, val_loader_dec, DEVICE, MLP_DEC_PAR[\"DEC_MODEL_PATH\"])\n\n\n\n#SAVING FINAL TRANSLATOR\nencoder_model = MLP_ENC_PAR[\"CREATE_MODEL\"](MLP_ENC_PAR)\nencoder_model.load_state_dict(torch.load(MLP_DEC_PAR[\"ENC_MODEL_PATH\"]))\n\ndecoder_model = MLP_DEC_PAR[\"CREATE_MODEL\"](MLP_DEC_PAR)\ndecoder_model.load_state_dict(torch.load(MLP_DEC_PAR[\"DEC_MODEL_PATH\"]))\n\n\n\ntranslator = create_AE_Translator(encoder_model, decoder_model)\nPath(TRANSLATOR_PATH).parent.mkdir(parents=True, exist_ok=True)\n\ntorch.save(translator.state_dict(), TRANSLATOR_PATH)\n\ndel decoder_model, encoder_model, translator\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T12:55:24.178329Z","iopub.execute_input":"2025-11-17T12:55:24.178594Z","iopub.status.idle":"2025-11-17T13:02:02.281361Z","shell.execute_reply.started":"2025-11-17T12:55:24.178577Z","shell.execute_reply":"2025-11-17T13:02:02.280561Z"}},"outputs":[{"name":"stdout","text":"\n2. Building model...\nLayerNorm [896, 1000, 1200, 1400]\nParameters: 7,810,737\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/30: 100%|██████████| 13/13 [00:15<00:00,  1.20s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Train=1.3298, Val=0.9870\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/30: 100%|██████████| 13/13 [00:15<00:00,  1.18s/it]\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([27500, 1536]) torch.Size([27500, 1536])\nEpoch 2: Train=0.9286, Val=0.8156, MRR@100=0.1399 {'mrr': 0.8408355700510836, 'ndcg': 0.880123241239267, 'recall_at_1': 0.7389818181818182, 'recall_at_3': 0.9378181818181818, 'recall_at_5': 0.9701454545454545, 'recall_at_10': 0.9895636363636363, 'recall_at_50': 0.9989818181818182, 'l2_dist': 26.111242294311523}\n  New best: 0.1399\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/30: 100%|██████████| 13/13 [00:15<00:00,  1.20s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Train=0.8192, Val=0.7531\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/30: 100%|██████████| 13/13 [00:15<00:00,  1.20s/it]\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([27500, 1536]) torch.Size([27500, 1536])\nEpoch 4: Train=0.7684, Val=0.7202, MRR@100=0.2551 {'mrr': 0.898170450478819, 'ndcg': 0.9235439175924549, 'recall_at_1': 0.8282545454545455, 'recall_at_3': 0.9662545454545455, 'recall_at_5': 0.9865090909090909, 'recall_at_10': 0.9946545454545455, 'recall_at_50': 0.9991272727272728, 'l2_dist': 26.001779556274414}\n  New best: 0.2551\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/30: 100%|██████████| 13/13 [00:15<00:00,  1.18s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5: Train=0.7389, Val=0.7016\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/30: 100%|██████████| 13/13 [00:15<00:00,  1.18s/it]\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([27500, 1536]) torch.Size([27500, 1536])\nEpoch 6: Train=0.7193, Val=0.6903, MRR@100=0.2991 {'mrr': 0.915180987751522, 'ndcg': 0.9363660907900478, 'recall_at_1': 0.8549818181818182, 'recall_at_3': 0.9750181818181818, 'recall_at_5': 0.9888727272727272, 'recall_at_10': 0.9951636363636364, 'recall_at_50': 0.9993090909090909, 'l2_dist': 26.01835060119629}\n  New best: 0.2991\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/30: 100%|██████████| 13/13 [00:15<00:00,  1.21s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7: Train=0.7091, Val=0.6830\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/30: 100%|██████████| 13/13 [00:15<00:00,  1.20s/it]\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([27500, 1536]) torch.Size([27500, 1536])\nEpoch 8: Train=0.6953, Val=0.6747, MRR@100=0.3199 {'mrr': 0.9268492956631826, 'ndcg': 0.9451406324602721, 'recall_at_1': 0.8738181818181818, 'recall_at_3': 0.9796363636363636, 'recall_at_5': 0.9903272727272727, 'recall_at_10': 0.9960363636363636, 'recall_at_50': 0.9992727272727273, 'l2_dist': 26.05483627319336}\n  New best: 0.3199\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/30: 100%|██████████| 13/13 [00:15<00:00,  1.18s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9: Train=0.6845, Val=0.6689\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/30: 100%|██████████| 13/13 [00:15<00:00,  1.18s/it]\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([27500, 1536]) torch.Size([27500, 1536])\nEpoch 10: Train=0.6795, Val=0.6691, MRR@100=0.3356 {'mrr': 0.9330453550379051, 'ndcg': 0.9497733969833199, 'recall_at_1': 0.8849090909090909, 'recall_at_3': 0.9802545454545455, 'recall_at_5': 0.990909090909091, 'recall_at_10': 0.996, 'recall_at_50': 0.9994181818181819, 'l2_dist': 26.088123321533203}\n  New best: 0.3356\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/30: 100%|██████████| 13/13 [00:15<00:00,  1.21s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11: Train=0.6726, Val=0.6628\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/30: 100%|██████████| 13/13 [00:15<00:00,  1.20s/it]\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([27500, 1536]) torch.Size([27500, 1536])\nEpoch 12: Train=0.6649, Val=0.6593, MRR@100=0.3460 {'mrr': 0.9352789850075122, 'ndcg': 0.95148012284062, 'recall_at_1': 0.888109090909091, 'recall_at_3': 0.981890909090909, 'recall_at_5': 0.9920363636363636, 'recall_at_10': 0.9963272727272727, 'recall_at_50': 0.9994545454545455, 'l2_dist': 26.09223175048828}\n  New best: 0.3460\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/30: 100%|██████████| 13/13 [00:15<00:00,  1.18s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13: Train=0.6581, Val=0.6547\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/30: 100%|██████████| 13/13 [00:15<00:00,  1.18s/it]\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([27500, 1536]) torch.Size([27500, 1536])\nEpoch 14: Train=0.6515, Val=0.6531, MRR@100=0.3553 {'mrr': 0.9393077548053709, 'ndcg': 0.9544826456575993, 'recall_at_1': 0.8951636363636364, 'recall_at_3': 0.9826181818181818, 'recall_at_5': 0.9919636363636364, 'recall_at_10': 0.9965454545454545, 'recall_at_50': 0.9994909090909091, 'l2_dist': 26.099769592285156}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/30: 100%|██████████| 13/13 [00:15<00:00,  1.19s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15: Train=0.6483, Val=0.6517\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/30: 100%|██████████| 13/13 [00:15<00:00,  1.18s/it]\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([27500, 1536]) torch.Size([27500, 1536])\nEpoch 16: Train=0.6440, Val=0.6493, MRR@100=0.3599 {'mrr': 0.9403679479564251, 'ndcg': 0.9552811249932324, 'recall_at_1': 0.8969454545454545, 'recall_at_3': 0.9836727272727273, 'recall_at_5': 0.9921818181818182, 'recall_at_10': 0.9962545454545455, 'recall_at_50': 0.9994181818181819, 'l2_dist': 26.1003360748291}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/30: 100%|██████████| 13/13 [00:15<00:00,  1.21s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17: Train=0.6387, Val=0.6480\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/30: 100%|██████████| 13/13 [00:15<00:00,  1.21s/it]\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([27500, 1536]) torch.Size([27500, 1536])\nEpoch 18: Train=0.6349, Val=0.6448, MRR@100=0.3703 {'mrr': 0.9429032805668198, 'ndcg': 0.9571669553383735, 'recall_at_1': 0.9013818181818182, 'recall_at_3': 0.9837454545454546, 'recall_at_5': 0.992, 'recall_at_10': 0.9965818181818182, 'recall_at_50': 0.9995272727272727, 'l2_dist': 26.103736877441406}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/30: 100%|██████████| 13/13 [00:15<00:00,  1.21s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19: Train=0.6301, Val=0.6437\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/30: 100%|██████████| 13/13 [00:15<00:00,  1.21s/it]\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([27500, 1536]) torch.Size([27500, 1536])\nEpoch 20: Train=0.6260, Val=0.6416, MRR@100=0.3765 {'mrr': 0.9439414502222384, 'ndcg': 0.9579480087010414, 'recall_at_1': 0.9032363636363636, 'recall_at_3': 0.9844727272727273, 'recall_at_5': 0.9922909090909091, 'recall_at_10': 0.9965090909090909, 'recall_at_50': 0.9993818181818181, 'l2_dist': 26.110321044921875}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21/30: 100%|██████████| 13/13 [00:15<00:00,  1.18s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 21: Train=0.6222, Val=0.6401\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22/30: 100%|██████████| 13/13 [00:15<00:00,  1.18s/it]\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([27500, 1536]) torch.Size([27500, 1536])\nEpoch 22: Train=0.6193, Val=0.6389, MRR@100=0.3807 {'mrr': 0.9437667990907244, 'ndcg': 0.9578132980797136, 'recall_at_1': 0.9028727272727273, 'recall_at_3': 0.9842545454545455, 'recall_at_5': 0.9924727272727273, 'recall_at_10': 0.9961454545454546, 'recall_at_50': 0.9993818181818181, 'l2_dist': 26.11033821105957}\nEarly stopping at epoch 22\nCreating ProjectionMLP\nLayerNorm [896, 1000, 1200, 1400]\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"\n\nprint(\"\\nGenerating test predictions...\")\ntest_data = load_data(\"/kaggle/input/aml-dataset/test/test/test.clean.npz\")\ntest_embds = torch.from_numpy(test_data['captions/embeddings']).float()\n\ntranslator  = create_AE_Translator(create_model(MLP_ENC_PAR), create_model(MLP_DEC_PAR))\ntranslator.load_state_dict(torch.load(TRANSLATOR_PATH))\n\ntranslator.eval()\nwith torch.no_grad():\n    test_clip_list = []\n    pred_embds = translator(test_embds.to(DEVICE)).cpu()\n    \nsubmission = generate_submission(test_data['captions/ids'], pred_embds, 'submission.csv')\nprint(f\"\\nSubmission saved to: submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T13:02:02.282273Z","iopub.execute_input":"2025-11-17T13:02:02.282551Z","iopub.status.idle":"2025-11-17T13:02:05.378058Z","shell.execute_reply.started":"2025-11-17T13:02:02.282525Z","shell.execute_reply":"2025-11-17T13:02:05.377260Z"}},"outputs":[{"name":"stdout","text":"\nGenerating test predictions...\n\n2. Building model...\nCreating ProjectionMLP\nParameters: 4,501,121\n\n2. Building model...\nLayerNorm [896, 1000, 1200, 1400]\nParameters: 7,810,737\nGenerating submission file...\n✓ Saved submission to submission.csv\n\nSubmission saved to: submission.csv\n","output_type":"stream"}],"execution_count":14}]}